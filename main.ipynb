{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import Accuracy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.transforms import ToTensor, PadTransform, Sequential, VocabTransform\n",
    "from tqdm.notebook import tqdm\n",
    "from utils import build_model, train, test, get_predictions\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "with open(\"./data/pos_data.txt\", encoding='UTF-8') as txt_file:\n",
    "    data = txt_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data \n",
    "data = list(\n",
    "    filter(\n",
    "        lambda x: x != [''],\n",
    "        map(\n",
    "            lambda x: x.rstrip().split('\\t'),\n",
    "            data\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect input and label data\n",
    "X, y = [], []\n",
    "for row in data:\n",
    "    if row[1] == '<beg>':\n",
    "        x_element, y_element = [], []\n",
    "        y_element.append(row[1])\n",
    "    elif row[1] == '<end>':\n",
    "        y_element.append(row[1])\n",
    "        X.append(x_element), y.append(y_element)\n",
    "    else:\n",
    "        y_element.append(row[2].split('|')[0])\n",
    "\n",
    "    x_element.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sent_lens = list(\n",
    "    map(\n",
    "        lambda x: len(x) - 2,\n",
    "        X\n",
    "    )\n",
    ")\n",
    "q_75 = int(np.quantile(all_sent_lens, q=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(\n",
    "    filter(\n",
    "        lambda x: len(x) <= q_75 + 2,\n",
    "        X\n",
    "    )\n",
    ")\n",
    "y = list(\n",
    "    filter(\n",
    "        lambda x: len(x) <= q_75 + 2,\n",
    "        y\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in inputs data 104763\n",
      "Number of parts of speech 20\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
    "vocab_X = build_vocab_from_iterator(X_train, specials=[\"<PAD>\", \"<UNK>\", \"<beg>\", \"<end>\"], special_first=[0, 1, 2, 3])\n",
    "vocab_X.set_default_index(1)\n",
    "vocab_y = build_vocab_from_iterator(y_train, specials=[\"<PAD>\", \"<UNK>\", \"<beg>\", \"<end>\"], special_first=[0, 1, 2, 3])\n",
    "vocab_y.set_default_index(1)\n",
    "print(f\"Number of tokens in inputs data {len(vocab_X)}\")\n",
    "print(f\"Number of parts of speech {len(vocab_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTTaggingDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data, X_transforms, y_transforms):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.X_transforms = X_transforms\n",
    "        self.y_transforms = y_transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_transforms(self.X_data[index]), self.y_transforms(self.y_data[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "X_transforms = Sequential(\n",
    "    VocabTransform(vocab_X),\n",
    "    ToTensor(0),\n",
    "    PadTransform(max_length=q_75 + 2, pad_value=0),\n",
    ")\n",
    "\n",
    "y_transforms = Sequential(\n",
    "    VocabTransform(vocab_y),\n",
    "    ToTensor(0),\n",
    "    PadTransform(max_length=q_75 + 2, pad_value=0),\n",
    ")\n",
    "\n",
    "train_dataset = POSTTaggingDataset(X_data=X_train, y_data=y_train, X_transforms=X_transforms, y_transforms=y_transforms)\n",
    "test_dataset = POSTTaggingDataset(X_data=X_test, y_data=y_test, X_transforms=X_transforms, y_transforms=y_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_config.yaml\", \"r\") as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = config[\"batch_size\"]\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "device = config[\"device\"] if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "lr = config[\"optimizer_lr\"]\n",
    "n_epochs = config[\"n_epochs\"]\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "model.to(device=device)\n",
    "writer = SummaryWriter(config[\"log_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559e854c646b4581831c223193470bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(n_epochs)):\n",
    "    train_loss = train(model,\n",
    "                       train_dataloader,\n",
    "                       criterion,\n",
    "                       optimizer,\n",
    "                       device=device)\n",
    "    val_loss, acc_score_val = test(\n",
    "        model, \n",
    "        test_dataloader,\n",
    "        criterion,\n",
    "        task_type='clf',\n",
    "        metric=[Accuracy(task=\"multiclass\", num_classes=len(vocab_y), ignore_index=0)],\n",
    "        device=device\n",
    "    )\n",
    "    train_loss, acc_score_train = test(\n",
    "        model, \n",
    "        train_dataloader,\n",
    "        criterion,\n",
    "        task_type='clf',\n",
    "        metric=[Accuracy(task=\"multiclass\", num_classes=len(vocab_y), ignore_index=0)],\n",
    "        train_or_test_mode=\"train\",\n",
    "        device=device\n",
    "    )\n",
    "    writer.add_scalar('Loss/train', train_loss / len(train_dataloader), epoch)\n",
    "    writer.add_scalar('Loss/test', val_loss / len(test_dataloader), epoch)\n",
    "    writer.add_scalar('Acc/train',acc_score_train[0], epoch)\n",
    "    writer.add_scalar('Acc/test', acc_score_val[0], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b8f997f1d43559167abab5eece01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true, y_pred = get_predictions(model, test_dataloader, device=device)\n",
    "y_without_pad_mask = y_true != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sample\n",
      "Final Accuracy-score = 0.9557736741588879\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00   2563801\n",
      "           2       1.00      1.00      1.00    620802\n",
      "           3       0.88      1.00      0.93    620802\n",
      "           4       0.27      0.98      0.42    801896\n",
      "           5       1.00      1.00      1.00    697183\n",
      "           6       0.68      0.91      0.78    695566\n",
      "           7       0.59      0.76      0.67    365388\n",
      "           8       0.98      0.99      0.98    345793\n",
      "           9       1.00      1.00      1.00    247222\n",
      "          10       0.94      0.94      0.94    231953\n",
      "          11       0.95      0.95      0.95    115777\n",
      "          12       0.97      0.90      0.93    105956\n",
      "          13       0.85      0.83      0.84     25943\n",
      "          14       0.53      0.16      0.24      4192\n",
      "          15       0.19      0.24      0.21      2443\n",
      "          16       0.71      0.25      0.37      1819\n",
      "          17       0.42      0.11      0.18      1696\n",
      "          18       0.67      0.40      0.50      1294\n",
      "          19       0.82      0.28      0.41        98\n",
      "\n",
      "    accuracy                           0.63   7449624\n",
      "   macro avg       0.76      0.67      0.65   7449624\n",
      "weighted avg       0.86      0.63      0.54   7449624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Test sample\")\n",
    "print(f\"Final Accuracy-score = {accuracy_score(y_true[y_without_pad_mask], y_pred[y_without_pad_mask])}\")\n",
    "print(classification_report(y_true, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model.state_dict(), f=\"weight.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: ['<beg>', '<UNK>', 'лучами', 'солнца', '<end>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "Model answer: ['<beg>', 'VERB', 'NOUN', 'NOUN', '<end>', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'NOUN']\n",
      "Ground Truth: ['<beg>', 'VERB', 'NOUN', 'NOUN', '<end>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "assert idx < len(test_dataset)\n",
    "x_test_element, y_test_element = test_dataset[idx]\n",
    "out = model(x_test_element.unsqueeze(0).to(device)).argmax(dim=1).cpu().flatten().numpy().tolist()\n",
    "print(f\"Input data: {vocab_X.lookup_tokens(x_test_element.tolist())}\")\n",
    "print(f\"Model answer: {vocab_y.lookup_tokens(out)}\")\n",
    "print(f\"Ground Truth: {vocab_y.lookup_tokens(y_test_element.tolist())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
